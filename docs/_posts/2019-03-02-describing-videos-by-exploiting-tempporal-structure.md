---
title: 基于时序结构的视频描述
author: 824zzy
date: 2019-03-02 12:00 +0800
comments: true
mathjax: true
categories: 
  - video captioning
tags:
  - video captioning
header:
  teaser: /assets/2019/2019-03-03-Mila.png
---



## 论文基本信息
1. 论文名：Describing Videos by Exploiting Temporal Structure

2. 论文链接：https://arxiv.org/pdf/1502.08029

3. 论文源码：
    - [https://github.com/tsenghungchen/SA-tensorflow](https://github.com/tsenghungchen/SA-tensorflow)
    
4. 关于笔记作者：
    - 朱正源,北京邮电大学研究生，研究方向为多模态与认知计算。  

---

## 论文推荐理由
本文是蒙特利尔大学发表在ICCV2015的研究成果，其主要创新点在于提出了时序结构并且利用注意力机制达到了在2015年的SOTA。通过3D-CNN捕捉视频局部信息和注意力机制捕捉全局信息相结合，可以全面提升模型效果。
其另一个重要成果是MVAD电影片段描述数据集，此[数据集](https://mila.quebec/en/publications/public-datasets/m-vad/)已经成为了当前视频描述领域主流的数据集。


---

## Describing Videos by Exploiting Temporal Structure

### 视频描述任务介绍：
根据视频生成单句的描述，一例胜千言：

![](http://ww1.sinaimg.cn/mw690/ca26ff18ly1fzcxfmvyxuj20si0hqqfg.jpg)

　　A monkey pulls a dog’s tail and is chased by the dog.

2015年较早的模型：
![LSTM-YT模型](http://ww1.sinaimg.cn/mw690/ca26ff18ly1fzcwjf53bsj214x0ksajx.jpg)

### 2015年之前的模型存在的问题
1. 输出的描述没有考虑到动态的**时序结构**。
2. 之前的模型利用一个特征向量来表示视频中的所有帧，导致无法识别视频中物体出现的**先后顺序**。


### 论文思路以及创新点
1. 通过局部和全局的时序结构来产生视频描述：

![](https://ws1.sinaimg.cn/large/ca26ff18ly1g0odfi9c82j20zk0eih1g.jpg)

  针对Decoder生成的每一个单词，模型都会关注视频中特定的某一帧。

2. 使用3-D CNN来捕捉视频中的动态时序特征。


### 模型结构设计

![](https://ws1.sinaimg.cn/large/ca26ff18ly1g0oe02i6tjj21c80k47e5.jpg)

- Encoder(3-D CNN + 2-D GoogLeNet)的设置：3 \* 3 \* 3 的三维卷积核，并且是3-D CNN在行为识别数据集上预训练好的。

每个卷积层后衔接ReLu激活函数和Local max-pooling， dropout参数设置为0.5。

![](https://ws1.sinaimg.cn/large/ca26ff18ly1g0oe2iz7iaj20v20ien2y.jpg)

- Decoder(LSTM)的设置：使用了additive attention作为注意力机制，下图为在两个数据集上的超参数设置：
![](https://ws1.sinaimg.cn/large/ca26ff18ly1g0osevs2qoj21620pwafz.jpg)

### 实验细节
#### 数据集
1. [Microsoft Research Video Description dataset](http://www.cs.utexas.edu/users/ml/clamp/videoDescription/)

> 1970条Youtobe视频片段：每条大约10到30秒，并且只包含了一个活动，其中没有对话。1200条用作训练，100条用作验证，670条用作测试。

2. [Montreal Video Annotation Dataset](https://mila.quebec/en/publications/public-datasets/m-vad/)

> 数据集包含从92部电影的49000个视频片段，并且每个视频片段都被标注了描述语句。

#### 评估指标
- BLEU

![](https://ws1.sinaimg.cn/large/ca26ff18ly1g0os1pgs0mj20qe0kgqh7.jpg)

- METEOR

![](https://ws1.sinaimg.cn/large/ca26ff18ly1g0os2ibx04j20su0mgh2g.jpg)

- CIDER
- Perplexity


#### 实验结果
1. 实验可视化
![实验结果](https://ws1.sinaimg.cn/large/ca26ff18ly1g0ornzkuylj220a144u0x.jpg)

柱状图表示每一帧生成对应颜色每个单词时的注意力权重。

2. 模型对比
![模型对比](https://ws1.sinaimg.cn/large/ca26ff18ly1g0ormgxp41j22120ggten.jpg)


### 引用与参考
- [Describing Videos by Exploiting Temporal Structure](https://arxiv.org/pdf/1502.08029)